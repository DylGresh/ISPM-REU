{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: March 14th 2024\n",
    "\n",
    "## Running Llama 3 8b Model on GPU vs on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "1. Make sure Python is installed on your system with the command `python --version`.\n",
    "2. Open a terminal and navigate to the directory this notebook is in and create a python virtual environment with the command `python -m venv .env`.\n",
    "3. Activate the virtual environment:\n",
    "  - If you're on Windows use `.env/Scripts/activate`.\n",
    "  - If you're on Linux use `source .env/bin/activate`.\n",
    "  - If you're on an ISPM Lab computer and don't have admin privileges, open VS Code and open a terminal from there, it should activate the environment for you. You'll know it has opened the virtual environment if a little pop-up notification appears in the bottom right (unless you've previously disabled this, in that case I'd imagine you're familiar with creating and activating virtual environments)\n",
    "4. Install the dependencies with `pip install 'transformers[torch]' 'optimum[onnxruntime]' jupyter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticating with HuggingFace\n",
    "\n",
    "1. Create an account on [HuggingFace](https://huggingface.co)\n",
    "2. If the model that you're using requires permission for you to access it (for example, all of meta-llama's models), navigate to their model and request access.\n",
    "3. While your request is being process or if there is no need for a request, you'll need to generate a token for authenticating with.\n",
    "   1. Click on your profile picture in the top right corner and click the Settings option from the drop down.\n",
    "   2. Click Access Tokens on the left\n",
    "   3. Click the New Token button and create a new token, the name doesn't matter but the Type should be `Write`\n",
    "   4. Copy the value of the token and return to your cli\n",
    "4. Run the command `huggingface-cli login` and when prompted paste your token in then hit enter. I like to save the token in my git credentials as well but it's not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Git-LFS\n",
    "\n",
    "Git-LFS (Git Large File Storage) is an extension of Git (same people) that allows storage of large files.\n",
    "\n",
    "1. Make sure git is already installed with `git -v`.\n",
    "   1. If git isn't installed already, download and setup git from [here](https://git-scm.com/downloads).\n",
    "2. Once you've confirmed that git is installed, download and install git-lfs by navigating to the [git lfs installation guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage?platform=linux) and follow the installation guide appropriate for your operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From ISPM computer with no GPU, run inference on pipeline with some optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from optimum.pipelines import pipeline\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the prompt we want to use on both the GPU and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Describe what a large language model is to me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the model down from HuggingFace and create an Optimum RunTime model from it and a Tokenizer from the transformers library\n",
    "# This might take a while since Llama hasn't already been transformed to an ORT model. I'm unsure if this is something that will always be a long process of if it's just a long process the first time.\n",
    "model = ORTModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the pipeline that will be used to perform the inference\n",
    "onnx_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, accelerator=\"ort\")\n",
    "\n",
    "# Define the context for the pipeline\n",
    "context = \"I'm a cybersecurity expert wanting to learn more about large language models.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the start time\n",
    "start_time = timer()\n",
    "\n",
    "# Perform the inference\n",
    "inference = onnx_pipe(prompt, context)\n",
    "\n",
    "# Get the stop time\n",
    "end_time = timer()\n",
    "elapsed_time = end_time - start_time  # Compute the elapsed time in seconds\n",
    "\n",
    "# Compute the hours, minutes, and seconds it took to perform the inference\n",
    "mins, secs = divmod(elapsed_time, 60)\n",
    "hours, mins = divmod(mins, 60)\n",
    "\n",
    "# Display results\n",
    "print(f\"Elapsed time (H:M:S): {hours:.0f}:{mins:.0f}:{secs:.5f}\")\n",
    "print(\"Output from model:\")\n",
    "print(f\"{inference}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ispm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
